{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a7041c",
   "metadata": {},
   "source": [
    "## 0. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fdfefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667ddf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/ocean/projects/cis250208p/shared/datasets\"\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"ccvl/3DSRBench\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11bcf622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['index', 'question', 'A', 'B', 'C', 'D', 'answer', 'category', 'image_source', 'image_url'],\n",
      "        num_rows: 5157\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75323e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a1166",
   "metadata": {},
   "source": [
    "## 1.APC Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a4a372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"  # or \"egl\" if CUDA drivers support EGL\n",
    "# os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"   # use EGL instead of OSMesa for NVIDIA drivers\n",
    "os.environ.pop(\"DISPLAY\", None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03dfe030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 23 11:02:48 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:16:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             39W /  300W |       1MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b031e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 98534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ab0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c1fcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"DISPLAY\"] =':1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # set GPU device\n",
    "sys.path.append(\"apc/vision_modules\")\n",
    "import yaml\n",
    "import re\n",
    "import requests\n",
    "from box import Box\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "# import APC pipeline\n",
    "from apc.apc_pipeline import APC\n",
    "from apc.utils import visualize_conversation, create_image_with_text\n",
    "\n",
    "# set device\n",
    "device_vlm = \"cuda:0\"\n",
    "device_vision = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "072e8cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded config Qwen/Qwen2.5-VL-7B-Instruct\n",
      "[INFO] Loaded config hidden_size=3584, model_type=qwen2_5_vl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e973f99ef34e7289991bf27006f01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded model type: qwen2_5_vl\n",
      "[INFO] Hidden size: 3584\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 2.44 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.21 GiB is allocated by PyTorch, and 149.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m Box(config)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# load APC pipeline\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m apc \u001b[38;5;241m=\u001b[39m \u001b[43mAPC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_vlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_vlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_vision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_vision\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/APC-VLM-idl-baseline/apc/apc_pipeline.py:71\u001b[0m, in \u001b[0;36mAPC.__init__\u001b[0;34m(self, config, device_vlm, device_vision)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[APC] Initialized VLM model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mvlm\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Initialize vision modules\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetection_module \u001b[38;5;241m=\u001b[39m \u001b[43mDetectionModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_vision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth_module \u001b[38;5;241m=\u001b[39m DepthModule(config, device_vision)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morientation_module \u001b[38;5;241m=\u001b[39m OrientationModule(config, device_vision)\n",
      "File \u001b[0;32m~/project/APC-VLM-idl-baseline/apc/vision_modules/detection.py:53\u001b[0m, in \u001b[0;36mDetectionModule.__init__\u001b[0;34m(self, config, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetection_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# print(\"* [INFO] Loaded GroundingDINO!\")\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Load SAM\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_model \u001b[38;5;241m=\u001b[39m \u001b[43msam_model_registry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_predictor \u001b[38;5;241m=\u001b[39m SamPredictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_model)\n",
      "File \u001b[0;32m/ocean/projects/cis250208p/ydinga/envs/apc_vlm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1369\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1367\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ocean/projects/cis250208p/ydinga/envs/apc_vlm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/ocean/projects/cis250208p/ydinga/envs/apc_vlm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 928 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/ocean/projects/cis250208p/ydinga/envs/apc_vlm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/ocean/projects/cis250208p/ydinga/envs/apc_vlm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:955\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 955\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "File \u001b[0;32m/ocean/projects/cis250208p/ydinga/envs/apc_vlm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1350\u001b[0m             device,\n\u001b[1;32m   1351\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1352\u001b[0m             non_blocking,\n\u001b[1;32m   1353\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1354\u001b[0m         )\n\u001b[0;32m-> 1355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 2.44 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 31.21 GiB is allocated by PyTorch, and 149.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "config_path = \"apc/configs/qwenvl2_5_7b_instruct.yaml\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "config = Box(config)\n",
    "\n",
    "# load APC pipeline\n",
    "apc = APC(config, device_vlm=device_vlm, device_vision=device_vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff9d2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(image_url):\n",
    "    response = requests.get(image_url, timeout=20)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\").resize((512, 512))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93c6e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from trimesh.viewer import windowed\n",
    "import io\n",
    "\n",
    "def headless_render(self, resolution=(256, 256), *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Fully compatible headless fallback for trimesh.Scene.save_image().\n",
    "    Returns raw PNG bytes (so downstream code expecting bytes won't break).\n",
    "    \"\"\"\n",
    "    # Create a neutral gray placeholder image\n",
    "    arr = np.ones((resolution[1], resolution[0], 3), dtype=np.uint8) * 127\n",
    "    img = Image.fromarray(arr)\n",
    "    \n",
    "    # Encode it as PNG bytes\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"PNG\")\n",
    "    png_bytes = buf.getvalue()\n",
    "    buf.close()\n",
    "    return png_bytes\n",
    "\n",
    "# Patch trimesh\n",
    "trimesh.Scene.save_image = headless_render\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d09aa423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# override trimesh.Scene.save_image to a safe headless version\n",
    "def headless_render(self, resolution=(256, 256), *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Fallback headless renderer that just returns a blank PNG.\n",
    "    It accepts all the same args/kwargs as trimesh.Scene.save_image.\n",
    "    \"\"\"\n",
    "    arr = np.ones((resolution[1], resolution[0], 3), dtype=np.uint8) * 127  # gray image\n",
    "    img = Image.fromarray(arr)\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"PNG\")\n",
    "    return buf.getvalue()\n",
    "\n",
    "# patch trimesh\n",
    "trimesh.Scene.save_image = headless_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c62bc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "\n",
    "# Suppress all general warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optionally, suppress only specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# For torch checkpoint warning specifically\n",
    "import torch.utils.checkpoint\n",
    "torch.utils.checkpoint.use_reentrant = False  # explicitly set new default (if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# random.seed(42)\n",
    "# indices = random.sample(range(len(ds)), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cbf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Balanced dataset size: 180\n",
      "['multi_object_parallel', 'orientation_viewpoint', 'multi_object_viewpoint_towards_object', 'orientation_viewpoint', 'multi_object_same_direction', 'orientation_in_front_of', 'orientation_on_the_left', 'multi_object_closer_to', 'multi_object_parallel', 'height_higher', 'location_above', 'orientation_viewpoint', 'orientation_on_the_left', 'multi_object_closer_to', 'orientation_in_front_of', 'multi_object_same_direction', 'orientation_viewpoint', 'location_closer_to_camera', 'height_higher', 'orientation_viewpoint']\n"
     ]
    }
   ],
   "source": [
    "# from datasets import concatenate_datasets\n",
    "\n",
    "# # how many total samples to select\n",
    "# total_samples = 180\n",
    "\n",
    "# # unique categories\n",
    "# categories = list(set(ds[\"category\"]))\n",
    "# samples_per_cat = total_samples // len(categories)\n",
    "\n",
    "# balanced_subsets = []\n",
    "\n",
    "# for cat in categories:\n",
    "#     subset = ds.filter(lambda x: x[\"category\"] == cat)\n",
    "#     n = min(samples_per_cat, len(subset))  # handle small categories\n",
    "#     subset = subset.shuffle(seed=42).select(range(n))\n",
    "#     balanced_subsets.append(subset)\n",
    "\n",
    "# # combine them and shuffle the final set\n",
    "# balanced_ds = concatenate_datasets(balanced_subsets).shuffle(seed=42)\n",
    "\n",
    "# print(f\"✅ Balanced dataset size: {len(balanced_ds)}\")\n",
    "# print(balanced_ds['category'][:20])  # preview the categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599d73253e364895837c0a13cbd89f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# balanced_ds.save_to_disk(\"test_dataset_150\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "ds = load_from_disk(\"test_dataset_150\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de674c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"test_dataset_150.txt\", \"w\") as f:\n",
    "#     for idx in balanced_ds[\"index\"]:\n",
    "#         f.write(str(idx) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ccc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = balanced_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fd8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 3DSRBench:   0%|                                         | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating 3DSRBench: 100%|███████████████████████████████| 180/180 [41:50<00:00, 13.95s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, example in enumerate(tqdm(ds, desc=\"Evaluating 3DSRBench\")):\n",
    "    image_url = example[\"image_url\"]\n",
    "    question = example[\"question\"]\n",
    "    options = [example[\"A\"], example[\"B\"], example[\"C\"], example[\"D\"]]\n",
    "    correct = example[\"answer\"]\n",
    "    category = example[\"category\"]\n",
    "\n",
    "    # Download image\n",
    "    image = download_image(image_url)\n",
    "\n",
    "    # plt.imshow(image)\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.title(f\"Index {i} — Category: {example['category']}\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # Build prompt\n",
    "    valid_options = [(chr(65 + j), opt) for j, opt in enumerate(options) if opt and str(opt).strip()]\n",
    "    options_text = \" or \".join([f\"{label}. {opt}\" for label, opt in valid_options])\n",
    "    prompt = f\"From the camera's point of view, {question.strip()} {options_text}, give the letter of the correct answer.\"\n",
    "\n",
    "    # print(\"Prompt:\")\n",
    "    # print(prompt)\n",
    "\n",
    "    image_with_text = create_image_with_text(image, \"[Q] \" + prompt, fontsize=20)\n",
    "    # image_with_text.save(f\"outputs/benchmark/3DSRBench_{i}_prompt.png\")\n",
    "\n",
    "    # Directory for saving intermediate results\n",
    "    save_dir = f\"outputs/benchmark/{category}_{i}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Run APC pipeline\n",
    "    response_text, _ = apc.run_apc(\n",
    "        image,\n",
    "        prompt,\n",
    "        trace_save_dir=save_dir,\n",
    "        perspective_prompt_type=\"visual\",\n",
    "        visualize_trace=False,\n",
    "        visualize_scene_abstraction=False,\n",
    "        return_conv_history=False,\n",
    "        logging=False,\n",
    "    )\n",
    "\n",
    "    # print(\"Response\", response_text)\n",
    "    # Extract predicted answer (search for 'A', 'B', 'C', 'D')\n",
    "    match = re.search(r\"\\b([ABCD])\\b\", response_text.upper())\n",
    "    pred_letter = match.group(1) if match else None\n",
    "\n",
    "    results.append({\n",
    "        \"index\": example[\"index\"],\n",
    "        \"category\": category,\n",
    "        \"question\": question,\n",
    "        \"prediction\": pred_letter,\n",
    "        \"answer\": correct,\n",
    "        \"is_correct\": pred_letter == correct,\n",
    "        \"response_text\": response_text,\n",
    "    })\n",
    "\n",
    "    # conv_viz = visualize_conversation(\n",
    "    #     conv_history,\n",
    "    #     width=900,\n",
    "    #     row_gap=0,\n",
    "    #     font_size=13,\n",
    "    #     image_max_width=180,\n",
    "    #     output_path=os.path.join(save_dir, \"conversation_viz.png\")\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"3DSRBench_raw_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a6bb0",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"3DSRBench_raw_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b68c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "\n",
    "# ---- Overall accuracy ----\n",
    "metrics[\"overall_accuracy\"] = df[\"is_correct\"].mean()\n",
    "\n",
    "# ---- Standard deviation of correctness (robustness) ----\n",
    "metrics[\"std_dev_accuracy\"] = df[\"is_correct\"].std()\n",
    "\n",
    "# ---- Category-wise accuracy ----\n",
    "cat_acc = df.groupby(\"category\")[\"is_correct\"].mean().to_dict()\n",
    "for cat, acc in cat_acc.items():\n",
    "    metrics[f\"acc_{cat}\"] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95d2e6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation Complete!\n",
      "                                                  0\n",
      "overall_accuracy                           0.333333\n",
      "std_dev_accuracy                           0.472719\n",
      "acc_height_higher                          0.400000\n",
      "acc_location_above                         0.333333\n",
      "acc_location_closer_to_camera              0.266667\n",
      "acc_location_next_to                       0.000000\n",
      "acc_multi_object_closer_to                 0.466667\n",
      "acc_multi_object_facing                    0.200000\n",
      "acc_multi_object_parallel                  0.533333\n",
      "acc_multi_object_same_direction            0.066667\n",
      "acc_multi_object_viewpoint_towards_object  0.200000\n",
      "acc_orientation_in_front_of                0.533333\n",
      "acc_orientation_on_the_left                0.666667\n",
      "acc_orientation_viewpoint                  0.333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =======================\n",
    "# Save results\n",
    "# =======================\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(\"3DSRBench_metrics_summary_3b.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Evaluation Complete!\")\n",
    "print(metrics_df.T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
